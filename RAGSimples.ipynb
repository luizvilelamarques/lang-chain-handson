{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0969cb8",
   "metadata": {},
   "source": [
    "RAG (Retrieval-Augmented Generation) Simples com Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21033ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c3741b1609544879183e0a9656959fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  46%|####6     | 41.9M/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d50ae854d814d0ebaedb6a6a40afd49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9fc90b6424d44b2a73fe1ddd6d8473d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96bf2d20211a48b4aca729ba4b6d15d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "619feff074634b248c44a9e026279fea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7b839e20f694b1383d71f23c056863f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- RAG em Ação com Groq (100% Open Source/Groq) ---\n",
      "--- Resposta da RAG ---\n",
      "De acordo com o contexto fornecido, o Groq é uma plataforma de inferência de IA que se destaca pela sua velocidade impressiva e é ideal para aplicações que exigem respostas rápidas, como diálogos de agentes virtuais. Sua relação com a LangChain é que oferece APIs que podem ser facilmente integradas com a LangChain, sugerindo que o Groq pode ser usado para executar modelos de linguagem em tempo real e que a LangChain pode utilizar as APIs do Groq para integrar com modelos de linguagem em tempo real.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Defina sua chave de API do Groq\n",
    "os.environ[\"GROQ_API_KEY\"] = \"sua chave de API aqui\"\n",
    "\n",
    "# 1. Crie os documentos (texto para o seu RAG)\n",
    "documento_completo = \"\"\"A LangChain é uma biblioteca poderosa para construir aplicações com LLMs. Ela facilita a conexão de LLMs com outras fontes de dados e agentes. A LangChain foi criada para ajudar no desenvolvimento de aplicações de IA, como chatbots e sistemas de perguntas e respostas. Um dos seus conceitos chave é a LangChain Expression Language (LCEL), que permite a criação de pipelines de forma intuitiva.\n",
    "O Groq, por outro lado, é uma plataforma de inferência de IA que se destaca pela sua velocidade impressionante. Ela usa hardware especializado para executar modelos de linguagem em tempo real, tornando-a ideal para aplicações que exigem respostas rápidas, como diálogos de agentes virtuais. O Groq oferece APIs que podem ser facilmente integradas com a LangChain.\"\"\"\n",
    "\n",
    "# 2. Divida o documento em partes menores\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documentos_divididos = text_splitter.create_documents([documento_completo])\n",
    "\n",
    "# 3. Crie os embeddings com um modelo do HuggingFace (não precisa de chave de API)\n",
    "# 'sentence-transformers/all-MiniLM-L6-v2' é um modelo popular e eficiente.\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "db = FAISS.from_documents(documentos_divididos, embeddings)\n",
    "\n",
    "# 4. Crie o Retrieval Chain com o Groq\n",
    "llm = ChatGroq(model_name=\"llama3-8b-8192\")\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Responda a pergunta baseada apenas no contexto fornecido.\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "Pergunta: {input}\"\"\")\n",
    "\n",
    "chain_documentos = create_stuff_documents_chain(llm, prompt)\n",
    "retriever = db.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, chain_documentos)\n",
    "\n",
    "# 5. Invoca a chain com uma pergunta sobre o documento\n",
    "print(\"--- RAG em Ação com Groq (100% Open Source/Groq) ---\")\n",
    "resposta = retrieval_chain.invoke({\"input\": \"O que é o Groq e qual sua relação com a LangChain?\"})\n",
    "print(\"--- Resposta da RAG ---\")\n",
    "print(resposta[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "groq-chain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

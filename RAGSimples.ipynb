{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0969cb8",
   "metadata": {},
   "source": [
    "RAG (Retrieval-Augmented Generation) Simples com Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21033ae1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_openai'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_openai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAIEmbeddings\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvectorstores\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FAISS\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtext_splitter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain_openai'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Defina sua chave de API do Groq\n",
    "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
    "\n",
    "# 1. Crie os documentos (texto para o seu RAG)\n",
    "documento_completo = \"\"\"A LangChain é uma biblioteca poderosa para construir aplicações com LLMs. Ela facilita a conexão de LLMs com outras fontes de dados e agentes. A LangChain foi criada para ajudar no desenvolvimento de aplicações de IA, como chatbots e sistemas de perguntas e respostas. Um dos seus conceitos chave é a LangChain Expression Language (LCEL), que permite a criação de pipelines de forma intuitiva.\n",
    "O Groq, por outro lado, é uma plataforma de inferência de IA que se destaca pela sua velocidade impressionante. Ela usa hardware especializado para executar modelos de linguagem em tempo real, tornando-a ideal para aplicações que exigem respostas rápidas, como diálogos de agentes virtuais. O Groq oferece APIs que podem ser facilmente integradas com a LangChain.\"\"\"\n",
    "\n",
    "# 2. Divida o documento em partes menores\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documentos_divididos = text_splitter.create_documents([documento_completo])\n",
    "\n",
    "# 3. Crie os embeddings com um modelo do HuggingFace (não precisa de chave de API)\n",
    "# 'sentence-transformers/all-MiniLM-L6-v2' é um modelo popular e eficiente.\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "db = FAISS.from_documents(documentos_divididos, embeddings)\n",
    "\n",
    "# 4. Crie o Retrieval Chain com o Groq\n",
    "llm = ChatGroq(model_name=\"llama3-8b-8192\")\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Responda a pergunta baseada apenas no contexto fornecido.\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "Pergunta: {input}\"\"\")\n",
    "\n",
    "chain_documentos = create_stuff_documents_chain(llm, prompt)\n",
    "retriever = db.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, chain_documentos)\n",
    "\n",
    "# 5. Invoca a chain com uma pergunta sobre o documento\n",
    "print(\"--- RAG em Ação com Groq (100% Open Source/Groq) ---\")\n",
    "resposta = retrieval_chain.invoke({\"input\": \"O que é o Groq e qual sua relação com a LangChain?\"})\n",
    "print(\"--- Resposta da RAG ---\")\n",
    "print(resposta[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "groq-chain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
